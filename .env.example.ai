# AI Provider Configuration
# ========================

# Provider Selection
# ------------------
# Options: "auto" (default), "openai", "llama-cpp"
# - auto: Tries OpenAI first, falls back to llama-cpp if available
# - openai: Use OpenAI only
# - llama-cpp: Use local LLM only
AI_PROVIDER=auto

# OpenAI Configuration
# --------------------
AI_INTEGRATIONS_OPENAI_API_KEY=your_openai_api_key_here
AI_INTEGRATIONS_OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini

# node-llama-cpp Configuration
# ----------------------------
# Path to your GGUF model file
# Download models from: https://huggingface.co/models?library=gguf
# Recommended: Llama-2-7B-Chat-GGUF, Mistral-7B-Instruct-GGUF
LLAMA_MODEL_PATH=./models/llama-2-7b-chat.Q4_K_M.gguf

# Context size (number of tokens the model can remember)
# Default: 4096, Higher values require more RAM
LLAMA_CONTEXT_SIZE=4096

# GPU layers to offload (for GPU acceleration)
# Default: 33 (auto-detect), Set to 0 for CPU-only
# Higher values = more GPU usage, faster inference
LLAMA_GPU_LAYERS=33

# Fallback Configuration
# ----------------------
# Enable automatic fallback to other providers
AI_FALLBACK_ENABLED=true

# Example Configurations
# ======================

# Configuration 1: OpenAI with local fallback (Recommended)
# AI_PROVIDER=auto
# AI_INTEGRATIONS_OPENAI_API_KEY=sk-...
# LLAMA_MODEL_PATH=./models/model.gguf
# AI_FALLBACK_ENABLED=true

# Configuration 2: Local LLM only (Privacy-focused)
# AI_PROVIDER=llama-cpp
# LLAMA_MODEL_PATH=./models/model.gguf
# LLAMA_GPU_LAYERS=33

# Configuration 3: OpenAI only (Simple setup)
# AI_PROVIDER=openai
# AI_INTEGRATIONS_OPENAI_API_KEY=sk-...
# OPENAI_MODEL=gpt-4o-mini

# Model Download Instructions
# ===========================
# 1. Create models directory: mkdir -p models
# 2. Download a GGUF model from HuggingFace:
#    - Llama-2-7B-Chat: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF
#    - Mistral-7B: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF
# 3. Place the .gguf file in the models directory
# 4. Update LLAMA_MODEL_PATH to point to your model

# Performance Tips
# ================
# - Q4_K_M quantization: Good balance of quality and speed
# - Q5_K_M quantization: Better quality, slower
# - Q8_0 quantization: Best quality, slowest, more RAM
# - Use GPU acceleration (LLAMA_GPU_LAYERS) for faster inference
# - Reduce LLAMA_CONTEXT_SIZE if you run out of memory
